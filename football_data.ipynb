{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Football data collection tutorial\n",
    "\n",
    "For this tutorial, we will be providing the means and methods on collecting data from various websites using webscraping techniques. In our case, it is football data from various sources that encompass different aspects needed for our project. \n",
    "\n",
    "A key note about the following code, we use requests and BeautifulSoup for each webscrape and will not discuss the loops with the exception of the first. Each loop is dependent on the website and you will need to determine the HTML elements your website is using and run your own loop. I will discuss issues when retreiving data from Wikipedia and code that was used to overcome it. \n",
    "\n",
    "Finally, all print statements are commented out as they were used for debugging purposes. Should you wish to use the code you are welcome to uncomment them to see what was producded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is importing the python packages we need in order to accomplish our task. In this instance we will need requests bs4 (BeatifulSoup), pandas, re and os. Once we have installed our packages, we wil use the `os` package to ensure we are in the correct directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andrewtamez/Desktop/NPS/OA3802_Comp3/Final_Project'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following portions of code, we first inspect the website we wish to pull data from. This is a key step as each website has their websites set up differently and use different html code to set them up.\n",
    "\n",
    "Given our website has a base url, we assign the url a variable that will allow us to concate the year in a loop later in the code. \n",
    "\n",
    "The variable below is how we established our base_url variable. Please insert the website you wish to explore in place for the one utilized by my team. \n",
    "\n",
    "`base_url = \"https://www.spotrac.com/nfl/position/_/year/\"`\n",
    "\n",
    "We then assign a variable for the years we are interested in. We also add in the salary caps for each year to allow the spending by position to be represented as a percentage. We then run our loop:\n",
    "\n",
    "1) The loop first appeneds the year as a string to the base url\n",
    "2) We make a GET request with `response.get(url)`\n",
    "3) Use `soup = BeautifulSoup(response.content, 'html.parser')` to parse out the HTML contect of the response created in step 2\n",
    "4) We then search for the `<div>` element using `table_wrapper = soup.find('div', id='table-wrapper')` and then search for `<table>` element using `table_wrapper.find('table')`\n",
    "5) The next portion `if table`, retrieves the table headers elements. We then extract the table rows with the `<tr>` elements. \n",
    "6) We then search each row in our `for row in rows` and extract all the cells, `<td>` elements, while stripping the whitespace. We then append to the cells to our data list. \n",
    "7) This list is then turned into a pandas dataframe and add in the year to each row. We also included a step that kes our team abbreviations to ensure we convert the names correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_abbreviations = {\"San Francisco 49ers\": \"SF\", \"Kansas City Chiefs\": \"KC\", \"Green Bay Packers\": \"GB\", \"Baltimore Ravens\": \"BAL\",\n",
    "    \"Dallas Cowboys\": \"DAL\", \"Miami Dolphins\": \"MIA\", \"Pittsburgh Steelers\": \"PIT\", \"Oakland Raiders\": \"OAK\", \"Las Vegas Raiders\": \"LV\",\n",
    "    \"Minnesota Vikings\": \"MIN\", \"New York Giants\": \"NYG\", \"New England Patriots\": \"NE\", \"Los Angeles Rams\": \"LAR\", \"Chicago Bears\": \"CHI\",\n",
    "    \"Philadelphia Eagles\": \"PHI\", \"Denver Broncos\": \"DEN\", \"Tampa Bay Buccaneers\": \"TB\", \"Seattle Seahawks\": \"SEA\", \"Indianapolis Colts\": \"IND\",\n",
    "    \"Washington Commanders\": \"WAS\", \"Washington Redskins\": \"WAS\", \"Buffalo Bills\": \"BUF\", \"Tennessee Titans\": \"TEN\", \"Atlanta Falcons\": \"ATL\",\n",
    "    \"Carolina Panthers\": \"CAR\", \"Arizona Cardinals\": \"ARI\", \"Cincinnati Bengals\": \"CIN\", \"Detroit Lions\": \"DET\", \"Jacksonville Jaguars\": \"JAX\",\n",
    "    \"New York Jets\": \"NYJ\", \"Houston Texans\": \"HOU\", \"Los Angeles Chargers\": \"LAC\", \"St. Louis Rams\": \"STL\", \"San Diego Chargers\": \"SD\",\n",
    "    \"New Orleans Saints\": \"NO\",}\n",
    "\n",
    "def scrape_spending_data():\n",
    "    base_url_spending = \"https://www.spotrac.com/nfl/position/_/year/\"\n",
    "    years = range(2011, 2025)\n",
    "    data_frames = {}\n",
    "\n",
    "    # Salary cap data in millions\n",
    "    salary_cap_data = {2024: 255.4, 2023: 224.8, 2022: 208.2,\n",
    "        2021: 182.5, 2020: 198.2, 2019: 188.2, 2018: 177.2,\n",
    "        2017: 167.0, 2016: 155.27, 2015: 143.28, 2014: 133.0,\n",
    "        2013: 123.0, 2012: 120.6, 2011: 120.0}\n",
    "\n",
    "    for year in years:\n",
    "        url = base_url_spending + str(year)\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table_wrapper = soup.find('div', id='table-wrapper')\n",
    "        if not table_wrapper:\n",
    "            print(f\"Missing data for year: {year}\")\n",
    "            continue\n",
    "        \n",
    "        table = table_wrapper.find('table')\n",
    "        if table:\n",
    "            headers = [header.text.strip() for header in table.find('thead').find_all('th')]\n",
    "            rows = table.find('tbody').find_all('tr')\n",
    "            \n",
    "            #print(f\"Year {year}: Found {len(rows)} rows of data.\")  \n",
    "\n",
    "            data = []\n",
    "            for row in rows:\n",
    "                cells = [cell.text.strip() for cell in row.find_all('td')]\n",
    "                if cells: \n",
    "                    data.append(cells)\n",
    "            \n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            df['Year'] = year\n",
    "            df['Team'] = df['Team'].apply(lambda x: team_abbreviations.get(x, x))\n",
    "\n",
    "            \n",
    "            for col in df.columns[1:-1]: \n",
    "                df[col] = df[col].replace('-', '0') \n",
    "                df[col] = df[col].str.replace('[$M]', '', regex=True).astype(float)\n",
    "                df[col] = df[col] / salary_cap_data[year]\n",
    "\n",
    "            data_frames[year] = df\n",
    "        #else:\n",
    "            #print(f\"Year {year}: No table found.\")\n",
    "\n",
    "    return data_frames\n",
    "\n",
    "#spending_data_frames = scrape_spending_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, we will not discuss the loops that are different for these codes as they follow simliar steps as the initial webscraping code above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_super_bowl_winners():\n",
    "    url = 'https://www.topendsports.com/events/super-bowl/winners-list.htm'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table')\n",
    "    rows = table.find_all('tr')\n",
    "    \n",
    "    super_bowl_winners = {}\n",
    "    for row in rows[1:]:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) >= 3:\n",
    "            year = int(cols[0].text.strip()) - 1  # Adjust for season year\n",
    "            winner = cols[2].text.strip()\n",
    "            short_name = team_abbreviations.get(winner, winner)\n",
    "            super_bowl_winners[year] = short_name\n",
    "    \n",
    "    return super_bowl_winners\n",
    "\n",
    "\n",
    "super_bowl_winners = scrape_super_bowl_winners()\n",
    "\n",
    "for year, df in spending_data_frames.items():\n",
    "    winner_team = super_bowl_winners.get(year)\n",
    "    \n",
    "    if winner_team:\n",
    "        df['SuperBowl_Win'] = df['Team'].apply(\n",
    "            lambda team: 1 if winner_team in team else 0)\n",
    "    else:\n",
    "        df['SuperBowl_Win'] = 0\n",
    "    \n",
    "    #print(f\"Year: {year}, Super Bowl Winner: {winner_team}\")\n",
    "    #print(df[['Team', 'SuperBowl_Win']].head())  # Display first few rows to check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_nfc_champions():\n",
    "    url = 'https://www.foxsports.com/stories/nfl/nfc-champions-complete-list-winners-year'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    champions = {}\n",
    "    for li in soup.find_all('li', class_='ff-h'):\n",
    "        span = li.find('span')\n",
    "        if span:\n",
    "            year_and_team = span.get_text(strip=True)\n",
    "            year, team = year_and_team.split(':', 1)\n",
    "            team = re.sub(r'\\s*\\(\\d+-\\d+\\)', '', team).strip()\n",
    "            champions[int(year.strip())] = team_abbreviations.get(team, team)\n",
    "\n",
    "    return champions\n",
    "\n",
    "def scrape_afc_champions():\n",
    "    url = 'https://www.foxsports.com/stories/nfl/afc-champions-complete-list-winners-year'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    champions = {}\n",
    "    for li in soup.find_all('li', class_='ff-h'):\n",
    "        span = li.find('span')\n",
    "        if span:\n",
    "            year_and_team = span.get_text(strip=True)\n",
    "            year, team = year_and_team.split(':', 1)\n",
    "            team = re.sub(r'\\s*\\(\\d+-\\d+\\)', '', team).strip()\n",
    "            champions[int(year.strip())] = team_abbreviations.get(team, team)\n",
    "\n",
    "    return champions\n",
    "\n",
    "nfc_champions = scrape_nfc_champions()\n",
    "afc_champions = scrape_afc_champions()\n",
    "\n",
    "#print(\"NFC Champions:\")\n",
    "#for year, team in nfc_champions.items():\n",
    "#    print(f\"Year: {year}, Team: {team}\")\n",
    "\n",
    "#print(\"\\nAFC Champions:\")\n",
    "#for year, team in afc_champions.items():\n",
    "#    print(f\"Year: {year}, Team: {team}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url_wins = 'https://www.nfl.com/standings/league/{}/REG'\n",
    "\n",
    "def scrape_wins(year):\n",
    "    url = base_url_wins.format(year)\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    table = soup.find('table')\n",
    "    rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "    wins_data = {}\n",
    "    for row in rows:\n",
    "        cols = row.find_all('td')\n",
    "        if len(cols) > 0:\n",
    "            team_name = cols[0].text.strip()\n",
    "            team_name = re.sub(r'\\n.*', '', team_name).strip()  # Remove any text after newline\n",
    "            team_name = team_abbreviations.get(team_name, team_name)  # Map to abbreviation\n",
    "            wins = int(cols[1].text.strip())\n",
    "            wins_data[team_name] = wins\n",
    "\n",
    "    return wins_data\n",
    "\n",
    "all_wins_data = {}\n",
    "for year in range(2011, 2025):\n",
    "    #print(f\"Scraping data for {year}...\")\n",
    "    all_wins_data[year] = scrape_wins(year)\n",
    "\n",
    "#for year, data in all_wins_data.items():\n",
    "    #print(f\"Wins data for {year}:\")\n",
    "    #for team, wins in data.items():\n",
    "        #print(f\"{team}: {wins} wins\")\n",
    "    #print(\"-\" * 40)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the codes above have been similar, we are adding in a final portion of code that grabs the teams that entered the playoffs for the years we are interested in. Most of the code is similar but we have run into issues. First, we must search for the `<wikitable>` elements within the wikipedia pages to find our data. \n",
    "\n",
    "We then look find the `<tr>` and `<td>` elements and pull our data. We have a few conditionals given the make up of the HTML code that will not be touched on as each page is unique in terms of HTML. \n",
    "\n",
    "I will discuss two new functions made specifically for the playoff wins. Pulling the data from wikipedia provided an additional step to ensure we only pulled the data we wanted. \n",
    "\n",
    "The first function `is_valid_team_name` that searches through the data we pulled. We are looking for any digits, values with `OT` in the name, or if the name was less than a length 2 as all the names are longer than 2. We then return a FALSE if these conditions are met, otherwise we return a TRUE. \n",
    "\n",
    "Finally, our last function then looks for the years we are interested in and uses the above function to pull the teams only if the values are TRUE. We filter them out and add them to a dictionary with the year as the key and the teams as the items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_playoff_teams():\n",
    "    url = \"https://en.wikipedia.org/wiki/NFL_playoff_results#All-time_playoff_records_(NFL/AFL)\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    playoff_data = {}\n",
    "    \n",
    "    tables = soup.find_all('table', class_='wikitable')\n",
    "    \n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        current_year = None\n",
    "        \n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            if cells:\n",
    "                first_cell_text = cells[0].get_text(strip=True)\n",
    "                if first_cell_text[:4].isdigit() and 2011 <= int(first_cell_text[:4]) <= 2024:\n",
    "                    current_year = int(first_cell_text[:4])\n",
    "                    if current_year not in playoff_data:\n",
    "                        playoff_data[current_year] = []\n",
    "                \n",
    "                if current_year:\n",
    "                    for cell in cells:\n",
    "                        team_links = cell.find_all('a', href=True)\n",
    "                        for link in team_links:\n",
    "                            if 'title' in link.attrs and 'cite_note' not in link['href']:\n",
    "                                team_name = link.get_text(strip=True)\n",
    "                                if is_valid_team_name(team_name):\n",
    "                                    if team_name not in playoff_data[current_year]:\n",
    "                                        playoff_data[current_year].append(team_name)\n",
    "    \n",
    "    return playoff_data\n",
    "\n",
    "def is_valid_team_name(name):\n",
    "    if ('OT' in name or any(char.isdigit() for char in name.split('-')) or \n",
    "        '–' in name or len(name) <= 2):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_filtered_playoff_teams(playoff_teams, team_abbreviations):\n",
    "    filtered_playoff_teams = {}\n",
    "    for year, teams in playoff_teams.items():\n",
    "        if 2011 <= year <= 2024:\n",
    "            valid_teams = [team_abbreviations.get(team, team) for team in teams if is_valid_team_name(team)]\n",
    "            filtered_playoff_teams[year] = valid_teams\n",
    "    return filtered_playoff_teams\n",
    "\n",
    "def main():\n",
    "    super_bowl_winners = scrape_super_bowl_winners()\n",
    "    nfc_champions = scrape_nfc_champions()\n",
    "    afc_champions = scrape_afc_champions()\n",
    "    spending_data_frames = scrape_spending_data()\n",
    "    playoff_teams = get_playoff_teams()\n",
    "    filtered_playoff_teams = get_filtered_playoff_teams(playoff_teams, team_abbreviations)\n",
    "    \n",
    "    for year, df in spending_data_frames.items():\n",
    "        wins_data = scrape_wins(year)\n",
    "        \n",
    "        df['Wins'] = df['Team'].map(wins_data).fillna(0).astype(int)\n",
    "        df['SuperBowl_Win'] = df['Team'].apply(lambda team: 1 if super_bowl_winners.get(year) == team else 0)\n",
    "        df['CC_Win'] = df['Team'].apply(lambda team: 1 if nfc_champions.get(year) == team or afc_champions.get(year) == team else 0)\n",
    "        df['Playoffs'] = df['Team'].apply(lambda team: 1 if team in filtered_playoff_teams.get(year, []) else 0)\n",
    "\n",
    "        #print(f\"Year: {year}\")\n",
    "        #print(df[['Team', 'Wins', 'SuperBowl_Win', 'CC_Win', 'Playoffs']].head())\n",
    "        #print(\"-\" * 40)\n",
    "\n",
    "    all_years_df = pd.concat(spending_data_frames.values())\n",
    "    all_years_df.to_csv('NFL_Positional_Spending_with_Championships_and_Wins.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
